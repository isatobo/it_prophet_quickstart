{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supporting Reading: \"Estimating the Entropy of Binary Time Series: Methodology, Some Theory and a Simulation Study\" https://arxiv.org/pdf/0802.4363v1.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Perform the same analysis as 1., but instead using _Volume Bars_, which are defined as: sampling every time a pre-defined amount of the security's units (shares, futures contracts, etc.) have been exchanged. For example, we could sample prices every time 100 BTC are traded, regardless of the time involved. Instead of having time as your \"clock\", we're using volume traded as our \"clock\" or index. In your case, use the 'quoteVolume' column when computing volume bars. Use 100 as your \"volume clock\" frequency for BTC, and use 2,000 for ETH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as sp_stats\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLONIEX_OHLCV_BASEURL = 'https://poloniex.com/public?command=returnChartData&currencyPair='\n",
    "\n",
    "\"\"\"\n",
    "https://poloniex.com/public?command=returnChartData&currencyPair=BTC_POT&start=1435699200&end=9999999999&period=14400\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def get_ohlcv_poloniex(pair='USDT_BTC', start=1435699200, end=9999999999, period=900):\n",
    "    \"\"\"\n",
    "    returns ohlcv data for poloniex as pandas dataframe\n",
    "    convert to unix timestamp using https://coderstoolbox.net/unixtimestamp/\n",
    "    :param pair: str pair on poloniex\n",
    "    :param start: int unix timestamp of beginning time\n",
    "    :param end: int unix timestamp of ending time\n",
    "    :param period: int candle width in seconds\n",
    "    :return: pandas df of ohlcv data from poloniex for specified pair, times, and period\n",
    "    \"\"\"\n",
    "    query = POLONIEX_OHLCV_BASEURL + pair + '&start=' + str(start) + '&end=' + str(end) + '&period=' + str(period)\n",
    "    resp = requests.get(query)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise requests.ApiError('GET /tasks/ {}'.format(resp.status_code))\n",
    "\n",
    "    return pd.DataFrame(resp.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantileEncoder:\n",
    "    \"\"\"\n",
    "    Performs quantile encoding for a real valued source.  The assigned value\n",
    "    for each bins is chosen as to minimize the mean square error introduced\n",
    "    by the quantization (conditional expectation within the bin).\n",
    "    Quantizer bins and mappings are computed from the empirical distribution\n",
    "    of the training data.\n",
    "    \"\"\"\n",
    "    def __init__(self, alphabet_size=2):\n",
    "        assert alphabet_size >= 2\n",
    "\n",
    "        self._quantiles = [n / float(alphabet_size)\n",
    "                           for n in range(alphabet_size + 1)]\n",
    "        self._bins = None\n",
    "        self._bin_mappings = None\n",
    "        self._suffix = '_right'\n",
    "\n",
    "    def fit(self, data: pd.Series):\n",
    "        \"\"\"\n",
    "        Fit the quantizer parameters from training data.\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.fit_transform(data)\n",
    "\n",
    "    def transform(self, data: pd.Series):\n",
    "        \"\"\"\n",
    "        Previously fit encoder performs the transformation on new data.\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert self._bins is not None\n",
    "        assert self._bin_mappings is not None\n",
    "\n",
    "        ser = pd.cut(data, self._bins, labels=False)\n",
    "        ser.name = 'labels'\n",
    "\n",
    "        df = pd.concat([ser, data], axis=1)\n",
    "        df_joined = df.join(self._bin_mappings, 'labels', rsuffix=self._suffix)\n",
    "\n",
    "        return df_joined[data.name + self._suffix]\n",
    "\n",
    "    def fit_transform(self, data: pd.Series):\n",
    "        \"\"\"\n",
    "        Train with then return the transformation of some data.\n",
    "        :param data:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ser, bins = pd.qcut(data, self._quantiles, retbins=True, labels=False)\n",
    "        ser.name = 'labels'\n",
    "\n",
    "        df = pd.concat([ser, data], axis=1)\n",
    "        bin_mappings = df.groupby('labels').mean()\n",
    "        df_joined = df.join(bin_mappings, 'labels', rsuffix=self._suffix)\n",
    "\n",
    "        self._bins = bins\n",
    "        self._bin_mappings = bin_mappings\n",
    "\n",
    "        return df_joined[data.name + self._suffix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plugIn(msg, w):\n",
    "    # Compute plug-in (ML) entropy rate\n",
    "    pmf = pmf1(msg, w)\n",
    "    out = - sum([pmf[i] * np.log2(pmf[i]) for i in pmf]) / w\n",
    "    return out, pmf\n",
    "\n",
    "def pmf1(msg, w):\n",
    "    # Compute the prob mass function for a 1D discrete RV\n",
    "    # len(msg)-w occurances\n",
    "    lib = {}\n",
    "    if not isinstance(msg, str): msg = ''.join(map(str, msg))\n",
    "    for i in range(w, len(msg)):\n",
    "        msg_ = msg[i-w: i]\n",
    "        if msg_ not in lib: \n",
    "            lib[msg_] = [i-w]\n",
    "        else: \n",
    "            lib[msg_] = lib[msg_] + [i-w]\n",
    "    pmf = float(len(msg) - w)\n",
    "    pmf = {i: len(lib[i])/pmf for i in lib}\n",
    "    return pmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message: 01100001, entropy: 0.84\n"
     ]
    }
   ],
   "source": [
    "class EntropyEstimatorLz:\n",
    "    \"\"\"\n",
    "    Kontoyiannis' LZ entropy estimate, 2013 version (centered window). Inverse\n",
    "    of the avg length of the shortest non-redundant substring. If non-redundant\n",
    "    substrings are short, the text is highly entropic. window==None for\n",
    "    expanding window, in which case\n",
    "    len(msg) % 2 == 0\n",
    "    If the end of msg is more relevant, try estimate_entropy(msg[::-1])\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def estimate_entropy(cls, *args, **kwargs):\n",
    "        return cls.konto(*args, **kwargs)['h']\n",
    "\n",
    "    @classmethod\n",
    "    def konto(cls, msg, window=None):\n",
    "        \"\"\"\n",
    "        :param msg:\n",
    "        :param window:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        out = {'num': 0, 'sum': 0, 'sub_str': []}\n",
    "        if not isinstance(msg, str):\n",
    "            msg = ''.join(map(str, msg))\n",
    "\n",
    "        if window is None:\n",
    "            points = range(1, len(msg) // 2 + 1)\n",
    "\n",
    "        else:\n",
    "            window = min(window, len(msg) // 2)\n",
    "            points = range(window, len(msg) - window + 1)\n",
    "\n",
    "        for i in points:\n",
    "            if window is None:\n",
    "                l, msg_ = cls.match_length(msg, i, i)\n",
    "                out['sum'] += math.log2(i + 1) / l\n",
    "\n",
    "            else:\n",
    "                l, msg_ = cls.match_length(msg, i, window)\n",
    "                out['sum'] += math.log2(window + 1) / l\n",
    "\n",
    "            out['sub_str'].append(msg_)\n",
    "            out['num'] += 1\n",
    "\n",
    "        out['h'] = (out['sum'] / out['num']) / math.log(2)\n",
    "        out['r'] = 1 - out['h'] / math.log2(len(msg))  # redundancy, 0 <= r <= 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def match_length(msg, i, n):\n",
    "        \"\"\"\n",
    "        Maximum matched length + 1, with overlap.\n",
    "        i >= n & len(msg) >= i + n\n",
    "        :param msg:\n",
    "        :param i:\n",
    "        :param n:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        sub_str = ''\n",
    "        for l in range(n):\n",
    "            msg1 = msg[i:i + l + 1]\n",
    "\n",
    "            for j in range(i - n, i):\n",
    "                msg0 = msg[j:j + l + 1]\n",
    "\n",
    "                if msg1 == msg0:\n",
    "                    sub_str = msg1\n",
    "                    break  # search for higher l.\n",
    "\n",
    "        return len(sub_str) + 1, sub_str  # matched length + 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Messages produces entropies of 0.97 and 0.84 as highlighted in\n",
    "    # \"Advances in Financial Machine Learning\" section 18.4\n",
    "    for m in ('11100001', '01100001'):\n",
    "        h = EntropyEstimatorLz.estimate_entropy(m) * math.log(2)\n",
    "print('message: %s, entropy: %.2f' % (m, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   close        date    high    low    open  quoteVolume    volume  \\\n",
      "0  225.0  1424373000    0.33  225.0    0.33     0.004444  0.999999   \n",
      "1  225.0  1424373300  225.00  225.0  225.00     0.000000  0.000000   \n",
      "2  225.0  1424373600  225.00  225.0  225.00     0.000000  0.000000   \n",
      "3  225.0  1424373900  225.00  225.0  225.00     0.000000  0.000000   \n",
      "4  225.0  1424374200  225.00  225.0  225.00     0.000000  0.000000   \n",
      "\n",
      "   weightedAverage  \n",
      "0            225.0  \n",
      "1            225.0  \n",
      "2            225.0  \n",
      "3            225.0  \n",
      "4            225.0  \n"
     ]
    }
   ],
   "source": [
    "df2 = get_ohlcv_poloniex(pair='USDT_BTC', start=0, end=9999999999, period=300)\n",
    "print(df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_ohlcv_poloniex(pair='USDT_BTC', start=0, end=9999999999, period=300)\n",
    "df['sumVolume'] = 0\n",
    "sumVolume = 0\n",
    "volumeBars = pd.DataFrame()\n",
    "for i, row in df.iterrows():\n",
    "    #df['volume'] += df['quoteVolume']\n",
    "    sumVolume += df.loc[i,'quoteVolume']\n",
    "    df.loc[i,'sumVolume'] = sumVolume\n",
    "    #df.loc[i,'sumVolume'] += df.loc[i,'quoteVolume'] + df.loc[i-1,'sumVolume']\n",
    "    #if df.loc[i,'volume'] % 100 == 0:\n",
    "       # volumeBars.append(df.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[(df['sumVolume'] % 100) == 0]\n",
    "#new_df = df.loc[(df['sumVolume'] % 100) == 0]\n",
    "print(new_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the price data every time we get to 100BTC traded \n",
    "# so, sample by day, then add all volumes until you get to 100, then save that entry's price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
